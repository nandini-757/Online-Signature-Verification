{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25f7f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted Features Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_duration</th>\n",
       "      <th>avg_speed</th>\n",
       "      <th>stroke_count</th>\n",
       "      <th>direction_changes</th>\n",
       "      <th>user</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1803.0</td>\n",
       "      <td>15.931164</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>USER1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1662.0</td>\n",
       "      <td>22.187465</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>USER1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1672.0</td>\n",
       "      <td>21.919729</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>USER1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1832.0</td>\n",
       "      <td>17.731168</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>USER1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1732.0</td>\n",
       "      <td>21.720494</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>USER1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_duration  avg_speed  stroke_count  direction_changes   user  label\n",
       "0          1803.0  15.931164             3                 14  USER1      1\n",
       "1          1662.0  22.187465             3                 19  USER1      1\n",
       "2          1672.0  21.919729             3                 19  USER1      1\n",
       "3          1832.0  17.731168             3                 17  USER1      1\n",
       "4          1732.0  21.720494             3                 15  USER1      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model Evaluation:\n",
      "Accuracy: 0.8\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        25\n",
      "           1       0.80      0.80      0.80        25\n",
      "\n",
      "    accuracy                           0.80        50\n",
      "   macro avg       0.80      0.80      0.80        50\n",
      "weighted avg       0.80      0.80      0.80        50\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[20  5]\n",
      " [ 5 20]]\n",
      "\n",
      "üéØ Model saved successfully at: dynamic_rf_model.pkl\n",
      "\n",
      "üîç Prediction Result: ‚ö†Ô∏è Forged Signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandini\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Step 1: Import Libraries\n",
    "# =============================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# =============================\n",
    "# Step 2: Dataset Path\n",
    "# =============================\n",
    "dataset_path = r\"C:\\Users\\Nandini\\osv-hybrid\\notebooks\\SVC2004_Sample\"  # <-- update this path\n",
    "\n",
    "# =============================\n",
    "# Step 3: Function to Read One Signature File\n",
    "# =============================\n",
    "def read_signature_file(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        n_points = int(lines[0].strip())\n",
    "        data = [list(map(float, line.strip().split())) for line in lines[1:n_points+1]]\n",
    "    df = pd.DataFrame(data, columns=['x', 'y', 'timestamp', 'button', 'azimuth', 'altitude', 'pressure'])\n",
    "    return df\n",
    "\n",
    "# =============================\n",
    "# Step 4: Extract Dynamic Features\n",
    "# =============================\n",
    "def extract_dynamic_features(df):\n",
    "    df = df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # 1Ô∏è‚É£ Total Duration\n",
    "    total_time = df['timestamp'].iloc[-1] - df['timestamp'].iloc[0]\n",
    "    \n",
    "    # 2Ô∏è‚É£ Average Speed\n",
    "    dx = np.diff(df['x'])\n",
    "    dy = np.diff(df['y'])\n",
    "    dt = np.diff(df['timestamp'])\n",
    "    distances = np.sqrt(dx**2 + dy**2)\n",
    "    speeds = distances / (dt + 1e-6)\n",
    "    avg_speed = np.mean(speeds)\n",
    "    \n",
    "    # 3Ô∏è‚É£ Stroke Count (pen-up ‚Üí pen-down)\n",
    "    button = df['button'].values\n",
    "    stroke_count = np.sum((button[1:] - button[:-1]) == 1)\n",
    "    \n",
    "    # 4Ô∏è‚É£ Direction Changes\n",
    "    angles = np.arctan2(dy, dx)\n",
    "    direction_changes = np.sum(np.abs(np.diff(angles)) > np.pi / 4)\n",
    "    \n",
    "    return {\n",
    "        'total_duration': total_time,\n",
    "        'avg_speed': avg_speed,\n",
    "        'stroke_count': stroke_count,\n",
    "        'direction_changes': direction_changes\n",
    "    }\n",
    "\n",
    "# =============================\n",
    "# Step 5: Extract Features from All Files\n",
    "# =============================\n",
    "all_features = []\n",
    "\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        user = filename.split(\"_\")[0]\n",
    "        sig_num = int(filename.split(\"_\")[1].split(\".\")[0])\n",
    "        label = 1 if sig_num <= 20 else 0  # 1 = genuine, 0 = forgery\n",
    "        df = read_signature_file(os.path.join(dataset_path, filename))\n",
    "        feat = extract_dynamic_features(df)\n",
    "        feat['user'] = user\n",
    "        feat['label'] = label\n",
    "        all_features.append(feat)\n",
    "\n",
    "features_df = pd.DataFrame(all_features)\n",
    "print(\"‚úÖ Extracted Features Sample:\")\n",
    "display(features_df.head())\n",
    "\n",
    "# =============================\n",
    "# Step 6: Split Dataset into Train & Test\n",
    "# =============================\n",
    "X = features_df[['total_duration', 'avg_speed', 'stroke_count', 'direction_changes']]\n",
    "y = features_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# =============================\n",
    "# Step 7: Train Random Forest Classifier\n",
    "# =============================\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# =============================\n",
    "# Step 8: Evaluate the Model\n",
    "# =============================\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(\"‚úÖ Model Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# =============================\n",
    "# Step 9: Save the Model\n",
    "# =============================\n",
    "model_path = \"dynamic_rf_model.pkl\"\n",
    "joblib.dump(rf_model, model_path)\n",
    "print(f\"\\nüéØ Model saved successfully at: {model_path}\")\n",
    "\n",
    "# =============================\n",
    "# Step 10: Test the Model on Random Input\n",
    "# =============================\n",
    "\n",
    "def predict_random_signature(df):\n",
    "    feat = extract_dynamic_features(df)\n",
    "    feat_values = np.array([[feat['total_duration'], feat['avg_speed'], feat['stroke_count'], feat['direction_changes']]])\n",
    "    pred = rf_model.predict(feat_values)[0]\n",
    "    return \"‚úÖ Genuine Signature\" if pred == 1 else \"‚ö†Ô∏è Forged Signature\"\n",
    "\n",
    "# Test with a random file\n",
    "test_file = os.path.join(dataset_path, \"USER1_25.txt\")  # choose any file (genuine or forgery)\n",
    "df_test = read_signature_file(test_file)\n",
    "result = predict_random_signature(df_test)\n",
    "print(\"\\nüîç Prediction Result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e4c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main datset trainig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92f007b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned and saved 1600 valid signature files.\n",
      "‚úÖ Extracted 1600 samples ‚Üí clean_signature_features.csv\n",
      "   Genuine: 800 | Forged: 800\n",
      "\n",
      "‚úÖ MODEL TRAINED SUCCESSFULLY ‚úÖ\n",
      "Accuracy: 0.7719\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.80      0.78       166\n",
      "           1       0.77      0.75      0.76       154\n",
      "\n",
      "    accuracy                           0.77       320\n",
      "   macro avg       0.77      0.77      0.77       320\n",
      "weighted avg       0.77      0.77      0.77       320\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[132  34]\n",
      " [ 39 115]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import re\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: CLEAN FILES\n",
    "# ============================================================\n",
    "def clean_task1_dataset(raw_folder, clean_folder):\n",
    "    os.makedirs(clean_folder, exist_ok=True)\n",
    "    all_files = glob.glob(f\"{raw_folder}/**/*.txt\", recursive=True)\n",
    "    valid = 0\n",
    "    \n",
    "    for f in all_files:\n",
    "        try:\n",
    "            with open(f, 'r', errors='ignore') as file:\n",
    "                lines = file.readlines()\n",
    "            if not lines:\n",
    "                continue\n",
    "            # Skip first line (number of points)\n",
    "            lines = lines[1:]\n",
    "            # Remove empty lines\n",
    "            lines = [ln for ln in lines if ln.strip()]\n",
    "            if len(lines) < 5:\n",
    "                continue\n",
    "            \n",
    "            # Save cleaned file\n",
    "            clean_path = f.replace(raw_folder, clean_folder)\n",
    "            os.makedirs(os.path.dirname(clean_path), exist_ok=True)\n",
    "            with open(clean_path, 'w') as wf:\n",
    "                wf.writelines(lines)\n",
    "            valid += 1\n",
    "        except Exception:\n",
    "            continue\n",
    "    print(f\"‚úÖ Cleaned and saved {valid} valid signature files.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: FEATURE EXTRACTION\n",
    "# ============================================================\n",
    "def extract_features_from_signature(df):\n",
    "    try:\n",
    "        # Keep only first 4 columns: X, Y, T, P\n",
    "        df = df.iloc[:, :4].copy()\n",
    "        df.columns = [\"X\", \"Y\", \"T\", \"P\"]\n",
    "        df = df.dropna()\n",
    "        if df.shape[0] < 5:\n",
    "            return None\n",
    "\n",
    "        # Ensure numeric\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "        if df[\"T\"].max() == df[\"T\"].min():\n",
    "            return None\n",
    "        \n",
    "        # Compute deltas\n",
    "        df[\"dx\"] = df[\"X\"].diff().fillna(0)\n",
    "        df[\"dy\"] = df[\"Y\"].diff().fillna(0)\n",
    "        df[\"dt\"] = df[\"T\"].diff().replace(0, np.nan).fillna(1)\n",
    "        \n",
    "        distances = np.sqrt(df[\"dx\"]**2 + df[\"dy\"]**2)\n",
    "        speeds = distances / df[\"dt\"]\n",
    "        \n",
    "        avg_speed = speeds.mean()\n",
    "        duration = df[\"T\"].iloc[-1] - df[\"T\"].iloc[0]\n",
    "        stroke_count = int(((df[\"P\"].shift(1) == 0) & (df[\"P\"] == 1)).sum())\n",
    "        \n",
    "        directions = np.arctan2(df[\"dy\"], df[\"dx\"])\n",
    "        direction_changes = np.sum(np.abs(np.diff(directions)) > np.pi / 4)\n",
    "        \n",
    "        return {\n",
    "            \"avg_speed\": avg_speed,\n",
    "            \"duration\": duration,\n",
    "            \"stroke_count\": stroke_count,\n",
    "            \"direction_changes\": direction_changes\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: BUILD DATASET WITH CORRECT LABELS\n",
    "# ============================================================\n",
    "def build_training_dataset(clean_folder):\n",
    "    files = glob.glob(f\"{clean_folder}/**/*.txt\", recursive=True)\n",
    "    features = []\n",
    "    genuine_count = forgery_count = 0\n",
    "\n",
    "    for f in files:\n",
    "        try:\n",
    "            # Extract user ID x and instance y from filename UxSy.txt\n",
    "            fname = os.path.basename(f)\n",
    "            match = re.match(r'U(\\d+)S(\\d+)\\.txt', fname, re.IGNORECASE)\n",
    "            if not match:\n",
    "                continue\n",
    "            user_id = int(match.group(1))\n",
    "            instance_id = int(match.group(2))\n",
    "\n",
    "            df = pd.read_csv(f, sep=r\"\\s+\", header=None)\n",
    "            feat = extract_features_from_signature(df)\n",
    "            if feat:\n",
    "                # Label: first 20 instances = genuine, rest = forged\n",
    "                if 1 <= instance_id <= 20:\n",
    "                    feat[\"label\"] = 1  # Genuine\n",
    "                    genuine_count += 1\n",
    "                else:\n",
    "                    feat[\"label\"] = 0  # Forged\n",
    "                    forgery_count += 1\n",
    "                features.append(feat)\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    feature_df = pd.DataFrame(features)\n",
    "    feature_df.to_csv(\"clean_signature_features.csv\", index=False)\n",
    "    print(f\"‚úÖ Extracted {len(feature_df)} samples ‚Üí clean_signature_features.csv\")\n",
    "    print(f\"   Genuine: {genuine_count} | Forged: {forgery_count}\")\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: TRAIN RANDOM FOREST\n",
    "# ============================================================\n",
    "def train_signature_model(df):\n",
    "    if df.empty or len(df[\"label\"].unique()) < 2:\n",
    "        print(\"‚ö†Ô∏è Not enough valid data to train model.\")\n",
    "        return\n",
    "    \n",
    "    X = df.drop(\"label\", axis=1)\n",
    "    y = df[\"label\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    \n",
    "    print(\"\\n‚úÖ MODEL TRAINED SUCCESSFULLY ‚úÖ\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, preds))\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, preds))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================\n",
    "RAW_PATH = r\"C:\\Users\\Nandini\\osv-hybrid\\notebooks\\Task1\"     # adjust path\n",
    "CLEAN_PATH = r\"C:\\Users\\Nandini\\osv-hybrid\\notebooks\\Task1_clean\"\n",
    "\n",
    "# 1. Clean raw files\n",
    "clean_task1_dataset(RAW_PATH, CLEAN_PATH)\n",
    "\n",
    "# 2. Extract features and build dataset\n",
    "df = build_training_dataset(CLEAN_PATH)\n",
    "\n",
    "# 3. Train Random Forest model\n",
    "train_signature_model(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc1f065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned and saved 1600 valid signature files.\n",
      "‚úÖ Extracted 1600 samples\n",
      "   Genuine: 800 | Forged: 800\n",
      "\n",
      "‚úÖ PHYSICS-BASED RF TRAINED\n",
      "Accuracy: 0.76875\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.78       166\n",
      "           1       0.76      0.75      0.76       154\n",
      "\n",
      "    accuracy                           0.77       320\n",
      "   macro avg       0.77      0.77      0.77       320\n",
      "weighted avg       0.77      0.77      0.77       320\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[130  36]\n",
      " [ 38 116]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import re\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: CLEAN FILES\n",
    "# ============================================================\n",
    "def clean_task1_dataset(raw_folder, clean_folder):\n",
    "    os.makedirs(clean_folder, exist_ok=True)\n",
    "    all_files = glob.glob(f\"{raw_folder}/*.txt\")\n",
    "    valid = 0\n",
    "\n",
    "    for f in all_files:\n",
    "        try:\n",
    "            with open(f, 'r', errors='ignore') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            if not lines:\n",
    "                continue\n",
    "\n",
    "            lines = lines[1:]  # skip point count\n",
    "            lines = [ln for ln in lines if ln.strip()]\n",
    "            if len(lines) < 6:\n",
    "                continue\n",
    "\n",
    "            clean_path = f.replace(raw_folder, clean_folder)\n",
    "            os.makedirs(os.path.dirname(clean_path), exist_ok=True)\n",
    "\n",
    "            with open(clean_path, 'w') as wf:\n",
    "                wf.writelines(lines)\n",
    "\n",
    "            valid += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(f\"‚úÖ Cleaned and saved {valid} valid signature files.\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: FEATURE EXTRACTION\n",
    "# ============================================================\n",
    "def extract_features_from_signature(df):\n",
    "    try:\n",
    "        df = df.iloc[:, :3].copy()\n",
    "        df.columns = [\"X\", \"Y\", \"T\"]\n",
    "        df = df.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "\n",
    "        if len(df) < 6:\n",
    "            return None\n",
    "\n",
    "        dx = np.diff(df[\"X\"])\n",
    "        dy = np.diff(df[\"Y\"])\n",
    "        dt = np.diff(df[\"T\"])\n",
    "        dt[dt == 0] = 1\n",
    "\n",
    "        # Velocity\n",
    "        v = np.sqrt(dx**2 + dy**2) / dt\n",
    "\n",
    "        # Acceleration\n",
    "        a = np.diff(v) / dt[1:]\n",
    "\n",
    "        # Direction\n",
    "        theta = np.arctan2(dy, dx)\n",
    "\n",
    "        # Angular velocity\n",
    "        w = np.diff(theta) / dt[1:]\n",
    "\n",
    "        # Angular acceleration\n",
    "        alpha = np.diff(w) / dt[2:]\n",
    "\n",
    "        # Simulated pressure\n",
    "        pressure = 1 / (v + 1e-6)\n",
    "\n",
    "        # Torque proxy\n",
    "        tau = pressure * v\n",
    "\n",
    "        def stats(arr):\n",
    "            return np.mean(arr), np.std(arr), np.max(arr)\n",
    "\n",
    "        stroke_count = np.sum(dt > np.percentile(dt, 90)) + 1\n",
    "        direction_changes = np.sum(np.abs(np.diff(theta)) > np.pi / 4)\n",
    "\n",
    "        return {\n",
    "            \"v_mean\": stats(v)[0],\n",
    "            \"v_std\": stats(v)[1],\n",
    "            \"v_max\": stats(v)[2],\n",
    "\n",
    "            \"a_mean\": stats(a)[0],\n",
    "            \"a_std\": stats(a)[1],\n",
    "            \"a_max\": stats(a)[2],\n",
    "\n",
    "            \"w_mean\": stats(w)[0],\n",
    "            \"w_std\": stats(w)[1],\n",
    "\n",
    "            \"alpha_mean\": stats(alpha)[0],\n",
    "            \"alpha_std\": stats(alpha)[1],\n",
    "\n",
    "            \"tau_mean\": stats(tau)[0],\n",
    "            \"tau_std\": stats(tau)[1],\n",
    "            \"tau_max\": stats(tau)[2],\n",
    "\n",
    "            \"stroke_count\": stroke_count,\n",
    "            \"direction_changes\": direction_changes\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: BUILD DATASET WITH CORRECT LABELS\n",
    "# ============================================================\n",
    "def build_training_dataset(clean_folder):\n",
    "    files = glob.glob(f\"{clean_folder}/*.txt\")\n",
    "    rows = []\n",
    "    genuine_count = forgery_count = 0\n",
    "\n",
    "    for f in files:\n",
    "        try:\n",
    "            fname = os.path.basename(f)\n",
    "            match = re.match(r'U(\\d+)S(\\d+)\\.txt', fname, re.IGNORECASE)\n",
    "            if not match:\n",
    "                continue\n",
    "\n",
    "            instance_id = int(match.group(2))\n",
    "\n",
    "            df = pd.read_csv(f, sep=r\"\\s+\", header=None)\n",
    "            features = extract_features_from_signature(df)\n",
    "\n",
    "            if features:\n",
    "                if 1 <= instance_id <= 20:\n",
    "                    features[\"label\"] = 1\n",
    "                    genuine_count += 1\n",
    "                else:\n",
    "                    features[\"label\"] = 0\n",
    "                    forgery_count += 1\n",
    "\n",
    "                rows.append(features)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    feature_df = pd.DataFrame(rows)\n",
    "    feature_df.to_csv(\"physics_signature_features.csv\", index=False)\n",
    "\n",
    "    print(f\"‚úÖ Extracted {len(feature_df)} samples\")\n",
    "    print(f\"   Genuine: {genuine_count} | Forged: {forgery_count}\")\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: TRAIN RANDOM FOREST\n",
    "# ============================================================\n",
    "def train_signature_model(df):\n",
    "    if df.empty or len(df[\"label\"].unique()) < 2:\n",
    "        print(\"‚ö† Not enough data to train.\")\n",
    "        return\n",
    "\n",
    "    X = df.drop(\"label\", axis=1)\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=12,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    print(\"\\n‚úÖ PHYSICS-BASED RF TRAINED\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "    print(\"\\nReport:\\n\", classification_report(y_test, preds))\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, preds))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================\n",
    "RAW_PATH = r\"C:\\Users\\Nandini\\osv-hybrid\\notebooks\\Task1\"\n",
    "CLEAN_PATH = r\"C:\\Users\\Nandini\\osv-hybrid\\notebooks\\Task2\"\n",
    "\n",
    "clean_task1_dataset(RAW_PATH, CLEAN_PATH)\n",
    "df = build_training_dataset(CLEAN_PATH)\n",
    "train_signature_model(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b4fd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned and saved 1600 valid signature files.\n",
      "‚úÖ Extracted 1600 samples\n",
      "   Genuine: 800 | Forged: 800\n",
      "‚úÖ Model saved as rf_model.pkl\n",
      "\n",
      "‚úÖ PHYSICS-BASED RF TRAINED\n",
      "Accuracy: 0.76875\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.78       166\n",
      "           1       0.76      0.75      0.76       154\n",
      "\n",
      "    accuracy                           0.77       320\n",
      "   macro avg       0.77      0.77      0.77       320\n",
      "weighted avg       0.77      0.77      0.77       320\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[130  36]\n",
      " [ 38 116]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import re\n",
    "import pickle  # ‚úÖ ADD THIS\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: CLEAN FILES\n",
    "# ============================================================\n",
    "def clean_task1_dataset(raw_folder, clean_folder):\n",
    "    os.makedirs(clean_folder, exist_ok=True)\n",
    "    all_files = glob.glob(f\"{raw_folder}/*.txt\")\n",
    "    valid = 0\n",
    "\n",
    "    for f in all_files:\n",
    "        try:\n",
    "            with open(f, 'r', errors='ignore') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            if not lines:\n",
    "                continue\n",
    "\n",
    "            lines = lines[1:]  # skip point count\n",
    "            lines = [ln for ln in lines if ln.strip()]\n",
    "            if len(lines) < 6:\n",
    "                continue\n",
    "\n",
    "            clean_path = f.replace(raw_folder, clean_folder)\n",
    "            os.makedirs(os.path.dirname(clean_path), exist_ok=True)\n",
    "\n",
    "            with open(clean_path, 'w') as wf:\n",
    "                wf.writelines(lines)\n",
    "\n",
    "            valid += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(f\"‚úÖ Cleaned and saved {valid} valid signature files.\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: FEATURE EXTRACTION\n",
    "# ============================================================\n",
    "def extract_features_from_signature(df):\n",
    "    try:\n",
    "        df = df.iloc[:, :3].copy()\n",
    "        df.columns = [\"X\", \"Y\", \"T\"]\n",
    "        df = df.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "\n",
    "        if len(df) < 6:\n",
    "            return None\n",
    "\n",
    "        dx = np.diff(df[\"X\"])\n",
    "        dy = np.diff(df[\"Y\"])\n",
    "        dt = np.diff(df[\"T\"])\n",
    "        dt[dt == 0] = 1\n",
    "\n",
    "        # Velocity\n",
    "        v = np.sqrt(dx**2 + dy**2) / dt\n",
    "\n",
    "        # Acceleration\n",
    "        a = np.diff(v) / dt[1:]\n",
    "\n",
    "        # Direction\n",
    "        theta = np.arctan2(dy, dx)\n",
    "\n",
    "        # Angular velocity\n",
    "        w = np.diff(theta) / dt[1:]\n",
    "\n",
    "        # Angular acceleration\n",
    "        alpha = np.diff(w) / dt[2:]\n",
    "\n",
    "        # Simulated pressure\n",
    "        pressure = 1 / (v + 1e-6)\n",
    "\n",
    "        # Torque proxy\n",
    "        tau = pressure * v\n",
    "\n",
    "        def stats(arr):\n",
    "            return np.mean(arr), np.std(arr), np.max(arr)\n",
    "\n",
    "        stroke_count = np.sum(dt > np.percentile(dt, 90)) + 1\n",
    "        direction_changes = np.sum(np.abs(np.diff(theta)) > np.pi / 4)\n",
    "\n",
    "        return {\n",
    "            \"v_mean\": stats(v)[0],\n",
    "            \"v_std\": stats(v)[1],\n",
    "            \"v_max\": stats(v)[2],\n",
    "\n",
    "            \"a_mean\": stats(a)[0],\n",
    "            \"a_std\": stats(a)[1],\n",
    "            \"a_max\": stats(a)[2],\n",
    "\n",
    "            \"w_mean\": stats(w)[0],\n",
    "            \"w_std\": stats(w)[1],\n",
    "\n",
    "            \"alpha_mean\": stats(alpha)[0],\n",
    "            \"alpha_std\": stats(alpha)[1],\n",
    "\n",
    "            \"tau_mean\": stats(tau)[0],\n",
    "            \"tau_std\": stats(tau)[1],\n",
    "            \"tau_max\": stats(tau)[2],\n",
    "\n",
    "            \"stroke_count\": stroke_count,\n",
    "            \"direction_changes\": direction_changes\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: BUILD DATASET WITH CORRECT LABELS\n",
    "# ============================================================\n",
    "def build_training_dataset(clean_folder):\n",
    "    files = glob.glob(f\"{clean_folder}/*.txt\")\n",
    "    rows = []\n",
    "    genuine_count = forgery_count = 0\n",
    "\n",
    "    for f in files:\n",
    "        try:\n",
    "            fname = os.path.basename(f)\n",
    "            match = re.match(r'U(\\d+)S(\\d+)\\.txt', fname, re.IGNORECASE)\n",
    "            if not match:\n",
    "                continue\n",
    "\n",
    "            instance_id = int(match.group(2))\n",
    "\n",
    "            df = pd.read_csv(f, sep=r\"\\s+\", header=None)\n",
    "            features = extract_features_from_signature(df)\n",
    "\n",
    "            if features:\n",
    "                if 1 <= instance_id <= 20:\n",
    "                    features[\"label\"] = 1\n",
    "                    genuine_count += 1\n",
    "                else:\n",
    "                    features[\"label\"] = 0\n",
    "                    forgery_count += 1\n",
    "\n",
    "                rows.append(features)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    feature_df = pd.DataFrame(rows)\n",
    "    feature_df.to_csv(\"physics_signature_features.csv\", index=False)\n",
    "\n",
    "    print(f\"‚úÖ Extracted {len(feature_df)} samples\")\n",
    "    print(f\"   Genuine: {genuine_count} | Forged: {forgery_count}\")\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: TRAIN RANDOM FOREST + SAVE PKL ‚úÖ\n",
    "# ============================================================\n",
    "def train_signature_model(df):\n",
    "    if df.empty or len(df[\"label\"].unique()) < 2:\n",
    "        print(\"‚ö† Not enough data to train.\")\n",
    "        return\n",
    "\n",
    "    X = df.drop(\"label\", axis=1)\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=12,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # ‚úÖ SAVE MODEL AS PKL\n",
    "    with open(\"rf_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    print(\"‚úÖ Model saved as rf_model.pkl\")\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    print(\"\\n‚úÖ PHYSICS-BASED RF TRAINED\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "    print(\"\\nReport:\\n\", classification_report(y_test, preds))\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, preds))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================\n",
    "RAW_PATH = r\"C:\\Users\\Nandini\\osv-hybrid\\notebooks\\Task1\"\n",
    "CLEAN_PATH = r\"C:\\Users\\Nandini\\osv-hybrid\\notebooks\\Task2\"\n",
    "\n",
    "clean_task1_dataset(RAW_PATH, CLEAN_PATH)\n",
    "df = build_training_dataset(CLEAN_PATH)\n",
    "train_signature_model(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
